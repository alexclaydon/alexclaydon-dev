<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-04-09">

<title>Relevant Context, Part 2: Ingestion – alexclaydon.dev</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-530bf8e59df25186f8bffdc63bad3503.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">alexclaydon.dev</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/alex-claydon-9369723/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/aclaydon_dev"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/alexclaydon"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Relevant Context, Part 2: Ingestion</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 9, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>“Ingestion” generically refers to the process of transforming a source document into a target format without - or minimising - loss of information.</p>
<p>The dominant document formats used for commercial legal agreements are Microsoft Word and Adobe PDF. Adobe’s PDF format can be further subdivided into “text”-based - where the characters on each page are represented as unicode characters - and “image”-based - where the contents of each page are effectively represented as an image file that must be passed through an optical chartacter recognition (OCR) process as part of the ingestion process. Each of these formats presents their own unique challenges.</p>
<p>Reliable, high-quality, at-scale ingestion of the wide variety of document formats in common use today is an enduringly challenging problem - so much so that it continues to support a sizeable enterprise market of sustainably profitable ingestion-as-a-service offerings tailored to specialist domain niches.</p>
<p>On the other hand, there are now a number of mature open source projects in the space that have changed what is achievable for smaller teams on more constrained budgets. With modest engineering resources, it is now possible - although it remains non-trivial - to set up and maintain a custom in-house ingestion pipeline for your particular domain.</p>
<section id="representations-suitable-for-use-with-llms" class="level2">
<h2 class="anchored" data-anchor-id="representations-suitable-for-use-with-llms">Representations Suitable for Use With LLMs</h2>
<p>Binary formats (such as <code>.docx</code> and <code>.pdf</code>) cannot be directly used in an LLM context window: they must first be transformed into a pure text representation. But that does not imply that we are constrained to only representing <em>textual information</em>: document structure and formatting from the source document - elements of which are just as important for human comprehension as the text itself - can also be retained <em>to the extent that they may be represented using text</em>.</p>
<p>Even binary formats used in “what you see is what you get” (WYSIWYG) editors such as Microsoft Word represent some of their structure in text directly visible to the end-user: clause numbering in a legal contract, for example. But things like bolding, heading sizes, page breaks, and so forth, are represented using non-textual means - effectively a separate metadata layer which sits alongside the text itself, rather than existing as part of it. Bits of structure and formatting which may be essential to the interpretation of their subject text will be lost to the extent that they cannot be conveted to a textual representation during the ingestion process.</p>
<p>Happily, there now exist a number of mature, comparatively standardised, text-only representation formats. All of these are in principle suitable for use in LLM context windows, with the better-known variants being naturally preferred for their ubiquity in LLM training datasets. These formats enable varying levels of structural and formatting complexity depending on the particular exigencies of your use-case.</p>
<p>Perhaps best known - even outside of technical circles - is <a href="https://daringfireball.net/projects/markdown/">Markdown</a>, which has been around since 2004 and was formalised into the <a href="https://en.wikipedia.org/wiki/Markdown">CommonMark</a> specification in 2014. Markdown’s biggest strength is its simplicity: the specification can be learned in afternoon. But simplicity is also its biggest limitation: primarily focussed on web publishing, Markdown struggles with representations of document structure which go beyond simple heading-level demarcation. Although it has been extended to meet some of these challenges, it remains a fundamentally limited choice for high-fedility, pure-text representation of legal contracts. While it has the twin advantages of being both human-readable and well-understood by LLMs, it fundamentally is not a good choice for a <em>canonical</em> textual representation of a legal contract.</p>
<p>It can be an excellent choice for certain LLM-backed point solutions where structure and formatting are less important; but in those cases, you should choose to transform a higher-quality, “lossless” representation of your document into Markdown as and when needed, rather than use Markdown itself as the canonical representation. Unlike transforming proprietary binary formats to pure-text representations, transforming between text-based representations is usually trivial.</p>
</section>
<section id="canonical-representation" class="level2">
<h2 class="anchored" data-anchor-id="canonical-representation">Canonical Representation</h2>
<p>Perhaps the best-known alternative pure-text representations are <a href="https://asciidoc.org/">Asciidoc</a> and <a href="https://docutils.sourceforge.io/rst.html">reStructuredText</a>, both of which are better suited to preserving complex document structure while remaining human-legible. LLMs also appear to be comfortable with XML-based formats - although the trade-off there is perhaps reduced legibility for humans. Should we pick one of these and call it a day?</p>
<p>There may seem a neat symmetry between ingesting into, and storing your documents in, the same format that will ultimately be used with your LLM-backed features. But there are some big trade-offs to this approach, and we think many of the benefits are largely illusory, given the existence of better alternatives.</p>
<p>Let’s take a step back and think about what we’re trying to achieve and what we’ve established:</p>
<ul>
<li>We want bulletproof reliability in the ingestion process: getting great results from our product features is hard enough without having to deal with junk input data created by a poor ingestion process.</li>
<li>We want to use our documents with LLMs without sacrificing important structure and formatting, certainly.</li>
<li>However, we also know that we will have uses for our documents which do not directly - or immediately - involve LLMs, and for which we may want to leverage document structure or features that cannot be represented in a simple text representation at all. Chief amongst these are what we have referred to in <a href="../../posts/2025-04-07/index.html">Part 1 of this series</a> as “Document and Chunk Enhancement” processing.</li>
<li>We will almost certainly also want to be able to work with the document, or parts of the document, as structured objects in code, rather than just partial strings.</li>
<li>Images cannot be losslessly represented using pure-text representations, and tables can be difficult to impossible to represent, depending on their complexity.</li>
<li>Transforming between “near-text” representations and Markdown / Asciidoc / reStructuredText is usually trivially easy - although it may not be lossless, depending on the target format chosen.</li>
</ul>
<p>Laying it all out like that, it’s clear that our actual constraint isn’t that meaningful document structure and formatting must be immediately representable in pure-text, but that it must be trivial to produce <em>arbitrary pure-text representations</em> from whatever interim canonical format we do choose to use. Loosening the requirement to store our canonical ingested documents in plain text means we can lean into a whole host of extra-textual features, allowing us to store and make sense of images and tables, work with structured objects in code, produce different target transformations for different purposes, and leverage the richer structure and formatting we have captured during ingestion in our metadata extraction and enhancement activities. We should assume that any format with these features is a reasonable candidate for our canonical representation, and indeed we should be ambivalent to any format possessing these features, all else being held equal.</p>
<p>As such, we should be much more interested in the robustness of the transformation performed by our ingestion software - including its performance across the major source formats commonly used in our domain - than the particulars of the chosen representation format (assuming it is high-resolution).</p>
</section>
<section id="recommended-tooling" class="level2">
<h2 class="anchored" data-anchor-id="recommended-tooling">Recommended Tooling</h2>
<p>Owing mainly to how challenging ingestion can be - particularly for documents which require OCR - historical limitations in the performance of open source tooling in the space, and unique business needs, in-house ingestion pipelines tend to involve a complex patchwork of processes. Despite recent advances - particularly in open source tooling - there is no single “off the shelf” tool that is capable of handling all of the edge cases even a team is likely to encounter when working with legal documents.</p>
<p>The following are our own recommendations for how we would put together a “greenfield” ingestion pipeline leveraging the current state of the art if we were starting from scratch today. We will update the below as and when there’s anything to add; notably, some of the LLM-backed solutions to knotty edge cases are evolving rapidly - we can reasonably expect to continue to see progress in the coming months.</p>
<section id="microsoft-word-documents-adobe-pdfs-not-requiring-ocr" class="level3">
<h3 class="anchored" data-anchor-id="microsoft-word-documents-adobe-pdfs-not-requiring-ocr">Microsoft Word Documents, Adobe PDFs Not Requiring OCR</h3>
<p>In our view, the starting point for most document ingestion should be either the Apache Foundation’s <a href="https://tika.apache.org/">Tika</a>, or IBM Research Zurich’s <a href="https://github.com/docling-project/docling">Docling</a>. Tika is by far the more mature option, but Docling - which might be a better fit for smaller teams given its more hands-off, opinionated approach - is a serious contender, in our experience demonstrating generally superior handling of PDFs, as well as providing structured extraction of tables and other PDF document features that Tika does not. We also like the structured object produced by Docling and its lossless JSON serialisation option. Some have found that Tika outperforms Docling on Microsoft Office documents, although our own experience is mixed. You should trial both for your use-cases.</p>
<p>There are other options - most notably, perhaps, Microsoft’s own <a href="https://github.com/microsoft/markitdown">MarkItDown</a> - but we are not recommending those at this time, since Tika and Docling - or some combination of the two - are such a compelling starting point.</p>
</section>
<section id="documents-requiring-ocr" class="level3">
<h3 class="anchored" data-anchor-id="documents-requiring-ocr">Documents Requiring OCR</h3>
<p>The complexity of OCR is driven in part by the wide variation in quality of the source documents. Printing quality, document age, whether the document has itself been reproduced from a poor copy (e.g., a Xerox of another document) - all impact legibility. As such, consistent, reliable, OCR remains challenging, with many edge cases.</p>
<p>As with non-OCR PDFs, Tika and Docling are good starting points, both coming bundled with OCR capabilities. Docling is also extensible, and is leaning into the nascent practice of <a href="https://huggingface.co/ds4sd/SmolDocling-256M-preview">leveraging multimodal LLMs</a> to improve OCR outcomes. Unlike some of the foundation model options below, which are only available over API, SmolDocling-256M-preview can be run on your own hardware. In the context of legal document ingestion, the privacy benefits of this are hard to overlook.</p>
<p>To the extent that either of those aren’t suitable for your use-case, we’ve also observed others having some success using freestanding multi-modal foundation LLMs over API as their primary ingestion tool. This approach has privacy implications, given that you will be sending the contents of each page over the wire to OpenAI, Google, etc. In addition, it can be challenging to preserve complete document structure using these approaches. But if all you really need is the text, and you have non-sensitive documents and/or a trusted inference provider, these are good “nuclear” options: they require lots of compute, but - if correctly implemented - can be very reliable. Certainly not something you’d want to use for every document coming in, but something perhaps to hold in reserve for where the cost is justified.</p>
<p>In practical terms, this can be done as follows:</p>
<ul>
<li>Extract each page of your PDF into a separate base-64 encoded image file (using, e.g., <a href="https://imagemagick.org/index.php">ImageMagick</a>)</li>
<li>If necessary, de-skew and/or crop the images (again, using, e.g., ImageMagick)</li>
<li>Pass the resulting image binary to one of the following multi-modal LLMs, all of which are considered state of the art for text extraction tasks: Google’s <a href="https://deepmind.google/technologies/gemini/flash/">Gemini Flash 2.0</a>, Anthropic’s <a href="https://www.anthropic.com/claude/sonnet">Claude Sonnet 3.7</a> or OpenAI’s GPT-4o., along with a prompt instructing the model to extract the text verbatim.</li>
<li>Stitch the resulting output back into a text-only representation (since much of the document structure will have been lost in the conversion)</li>
</ul>
<p>Note that <a href="https://mistral.ai/news/mistral-ocr">Mistral</a> also has a specialised endpoint available for OCR-related tasks. We haven’t personally used it, and it faces the same privacy-related concerns as any other LLM over inference API, but it does claim better handling of document structure versus the general-purpose multi-modal LLMs considered above.</p>
<p>Finally, if you have the hardware for it and can’t send your documents to your API inference provider, <a href="https://huggingface.co/Qwen/Qwen2.5-32B">Qwen 2.5 32b</a> is widely considered to be state of the art for text extraction tasks within the constraint of common destkop GPU VRAM limits.</p>


</section>
</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/alexclaydon\.dev");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Alex Claydon</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>
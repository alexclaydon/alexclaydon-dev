<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-04-02">

<title>Agentic RAG, Part 1: A Primer – alexclaydon.dev</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5f9854fdde466ae1e0b48e313c1a116e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">alexclaydon.dev</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/alex-claydon-9369723/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/aclaydon_dev"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/alexclaydon"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Agentic RAG, Part 1: A Primer</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 2, 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">April 7, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<p>Swapping single LLM calls for LLM-backed “agents” leads to materially improved outcomes on retrieval-augmented generation (“RAG”) tasks for legal document analysis. While additional costs are incurred, the approach is already economic for a wide range of B2B use cases and we expect token costs to continue trending downward. The substitution itself is relatively uncomplicated from an engineering standpoint, and eases the prompt engineering burden on domain experts - especially in smaller teams. While there is an impact on inference latency (that needs to be managed in the user experience), the improved outcomes are more than worth it for high value, high risk use cases such as legal analysis.</p>
<p>This post - Part 1 - aims to help readers situate agentic RAG in their product stack by considering technological constraints, trade-offs and the likely direction of developments going forward. Part 2, to follow, will compare and contrast two illustrative implementations of agentic behaviour in the context of legal document analysis using the <code>smolagents</code> and <code>DSpy</code> libraries.</p>
<section id="context-and-technological-constraints" class="level2">
<h2 class="anchored" data-anchor-id="context-and-technological-constraints">Context and Technological Constraints</h2>
<p>LLM “agent” frameworks built around large parameter-count foundation model inference over API received a lot of attention during 2023-2024. As with LLMs more generally, users were happy to trade the customisability and privacy of open source models and in-house inference stacks for the vastly more capable larger models and the freedom from having to run their own inference stacks on scarce hardware. Consumer GPU supply was still constrained by pandemic-era knock-on, and most new production capacity was (and is) going to data centre cards. GPT3.5 over API was often radically cheaper than running any of Meta’s early Llama models in-house, for instance, as well as being more performant. That cost equation remains true today, although smaller open source models in the hands of knowledgeable practitioners are now genuinely useful for a variety of use cases.</p>
<p>Other trade-offs were also accepted - the lack of context-free grammar support, control over logit bias, and the entire post-training and RLHF process, for instance - because the performance gap was so compelling. Function calling support, in particular, was crucial to bridging the gap. While reliability was initially an issue, those concerns have now largely been put to rest even for lower parameter-count LLMs.</p>
<p>The effect of these conditions on efforts to spin up useful LLM-backed agents has only recently become clear, and may provide an interesting signal as to where the current big model and inference providers - such as OpenAI - see value being created and captured.</p>
</section>
<section id="why-agency-matters" class="level2">
<h2 class="anchored" data-anchor-id="why-agency-matters">Why “Agency” Matters</h2>
<p>Attempts to define what constitutes an “agent” under the new regime are famously fraught.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> The blue-sky prospects are intriguing, but when it comes to using LLMs in legal document analysis, arguably the most compelling application in the present is in improving RAG outcomes.</p>
<p>We’re working on a separate post on the merits of RAG systems versus alternative approaches and on preparing your legal documents for use with such systems. For the purposes of this post, suffice to say that the inherent ability of agents to “reflect” and course-correct during the analytical process is an excellent mitigant to the inherent limitations of even current state-of-the-art approaches to RAG.</p>
<p>Agentic RAG is robust to:</p>
<ul>
<li>a wide variety of applications;</li>
<li>compared to single LLM calls, a wider range of approaches to document preparation; and</li>
<li>compared to single LLM calls, a wider range of variation in input prompt quality,</li>
</ul>
<p>potentially allowing teams to depreciate engineering and implementation costs across a range of product features.</p>
<p>The principal downsides are:</p>
<ul>
<li>increased latency; and</li>
<li>increased inference costs,</li>
</ul>
<p>although we expect technological advances will continue to improve both of these.</p>
<p>Given the ease of substituting existing RAG systems with an agentic RAG approach, we expect agentic RAG to largely replace most current approaches based on carefully crafted system prompts and single LLM calls in most quality-sensitive domains.</p>
</section>
<section id="comparison-with-single-call-approaches" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-single-call-approaches">Comparison with Single-Call Approaches</h2>
<p>All of which papers over perhaps the biggest technical question: how does it work?</p>
<p>A good starting point might be comparison against canonical “single LLM call” approaches to analystical queries in the legal domain. A user is interested in knowing something about a document that requires more than just searching: they need some kind of synthesis, and analysis, of the contents of the document or documents in order to satisfy an information need.</p>
<p>A domain specialist - a lawyer or legal engineer, perhaps - will have prepared the model in advance by carefully crafting a system prompt defining a common (as in: used for a variety of applications) specification to help guide the model’s spend of reasoning tokens: “Do this, don’t do that; focus on this, ignore that; contracts provided to you will be governed by English law”. More sophisticated approaches may craft this “framing” prompt dynamically, from information extracted from the document in advance of the user sitting down to interact with the system: “The parties are A and B, incorporated in the United Kingdom; the subject matter is a lease of commercial property located in Manchester, England; disputes shall be resolved through arbitration”. This lightens the load on the end user, who can simply sit down and say: “Summarise the payment terms with a particular focus on the consequences of a failure to pay on time”.</p>
<p>The process may not <em>literally</em> be single-call: many systems are likely to perform some degree of error correction and/or post-processing on the result sent back by the LLM. The more cycles and tokens you spend on error correction and post-processing, the more a single-call approach starts to resemble a “loop” in an “untrained” agentic system - particularly if control over the determination of when a response is ready to be seen by the user is largely handed over to the model. So in reality, when it comes to untrained agent systems, there’s a spectrum, with agentic systems handing near-complete control over decision-making to a “managing” agent. “Trained” agent systems are altogether different - something we’ll explore towards the end of the article.</p>
<p>Careful crafting of the system prompt remains an important step. Teams can and should be creating context and/or document specific framing prompts dynamically from information pre-extracted from the document(s). But by empowering the agent to more aggressively decompose the problem and make active judgements about quality - as we will see below - results in materially improved outcomes and consistency.</p>
<p>We’ll consider a concrete implementation of an untrained agent below, but first, it’s worth looking at developments in the LLM agent space in the first few months of 2025, which have thrown into sharp relief an emerging two-track system for LLM agents and a concerted attempt by model and inference providers - OpenAI in particular - to try to re-capture some of the value lost to the preponderance of high-quality open-source models.</p>
</section>
<section id="agency-untrained-or-trained" class="level2">
<h2 class="anchored" data-anchor-id="agency-untrained-or-trained">Agency: Untrained or Trained?</h2>
<p>Limited foundation model customisability has led to most popular open-source frameworks modelling their agentic approaches as directed cyclic or acyclic graphs - an approach common in robotic process automation (RPA) and workflow systems. This is effectively the “untrained agent” end of the spectrum discussed above.</p>
<p>Different frameworks abstract different aspects of the process of building the graph, so to speak, but all of them rely on establishing and exploiting a manager/managed relationship between multiple unrelated LLM call contexts. Though these approaches have proven fruitful, they create agency through artifice: the models themselves have no particular internal specialisation in performing agentic behaviours, so we create the desired effect with duelling tokens. Encapsulating relevant context analysis behind the “managed” LLM’s context window, and the output approval process behind that of the “manager” LLM is part of the point, but it is not without trade-offs. The extent of those trade-offs depends in part on the concrete implementation and use-case, and can be hard to measure empirically. But until this year, the point was moot: there was no other way for end-users of the most powerful LLMs provided over inference API to approach the task.</p>
<p>On 2 February 2025, OpenAI introduced the “Deep Research” mode in their B2C-facing ChatGPT apps. The rollout to Pro and Plus customers only finished recently, on 25 February. With the caveat that we cannot know for certain how OpenAI is doing what they do - since Deep Research was developed, like all of OpenAI’s products, behind closed doors - informed consensus is that the approach likely involves post-training the LLM with a reward function to select for, and enhance, desirable agentic behaviours such as reflection and error correction as part of the token generation process. Unlike untrained agents, which produce one “cycle” of thought and then are asked by a separate LLM to assess and reconsider their output, the process in trained agents occurs internally: from the user perspective, a question is asked and a high-quality response returned - albeit with higher latency than in single-call scenarios. Decisions on how to improve a piece of analysis, and when it is good enough to return to the user, are all handled by the same LLM, in the same context window.</p>
<p>It is early days for Deep Research. Competitors such as Google and Perplexity have released product features with similar names and behaviour that appears superficially similar to OpenAI’s offering. While it is too early for empirical confirmation, subjectively, the ouput produced by a trained agent - or, at least, OpenAI’s Deep Research - is head and shoulders above anything currently possible with untrained agents. OpenAI has not indicated whether a “Deep Research”-like feature will be made available through its API offering. All current signs seem to indicate that features like this represent OpenAI’s attempt to move up the value stack and claw back some of the value lost competitors and the recent spate of incredibly capable open-source models. As such, we would not be surprised if Deep Research continues to be absent from the API for the foreseeable future - at least until another provider commoditises a similarly compelling feature. We think it unlikely that features like this will form the basis of a sustainable moat for OpenAI. The story of the past two years has been that well-capitlised pathfinders like OpenAI are quickly swamped by fast-followers such as Meta and DeepSeek. But, at least for now, those of us implementing agentic behaviours into our products - including for RAG - must make do with existing untrained approaches.</p>
<p>You can find interesting in-depth discussion of the “trained vs untrained” dichotomy, and what it may mean for value creation and capture going forward, <a href="https://leehanchung.github.io/blogs/2025/02/26/deep-research/">here</a> and <a href="https://vintagedata.org/blog/posts/designing-llm-agents">here</a>.</p>
</section>
<section id="untrained-agent-implementations" class="level2">
<h2 class="anchored" data-anchor-id="untrained-agent-implementations">Untrained Agent Implementations</h2>
<p>In our next post, we propose to walk through two contrasting implementations of agentic RAG in some detail, in the hope that readers may find the concrete examples useful in implementing their own agentic RAG pipelines.</p>
<p>There are far too many open-source frameworks enabling agentic behaviour (for some definition of “agentic”) to cover in detail in one post. In any event, the similarity of approaches - a product of them all being confined to “untrained” approaches - would render such exercise of dubious value.</p>
<p>Instead, we intend to take a high-level look at just two: Hugging Face’s <code>smolagents</code>, and Stanford NLP’s <code>DSpy</code>.</p>
<p><code>smolagents</code><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> is notable for 3 reasons. First, Hugging Face is a giant in the open source machine learning space, thanks in large part to its widely used <code>transformers</code><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> library and its community model and dataset sharing platform, Hugging Face Hub. Second, <code>smolagents</code> itself is committed to being a “low-abstraction” library - unlike, say, <code>LangChain</code><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. This makes it a good fit for prototyping, easy to integrate and - most importantly for our purposes - the code is easy to read and easy to reason about. Third, <code>smolagents</code> takes a somewhat novel approach to writing tool definitions that readers may not have seen before<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>: instead of JSON, actions are written in Python code. In addition, Hugging Face recently published a blog post covering their own toy untrained re-implementation of OpenAI’s Deep Research feature in the <code>smolagents</code> library.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<p>Stanford NLP’s <code>DSpy</code><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> is interesting for different reasons, with the chosen approach contrasting nicely with <code>smolagents</code>. <code>DSpy</code> was launched in late 2023 with an Arxiv paper<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> that made waves. <code>DSpy</code>’s main value proposition has always been its focus on automated pipeline and prompt optimisation. <code>DSPy</code> approach is proving particularly impactful in the context of setting up untrained agents - including for RAG. Since the links between “manager” and “managed” LLMs in an agentic arrangement are, essentially, prompts, the brittle nature of prompts produced using traditional prompt engineering techniques has weighed heavily on performance.</p>
<p>We look forward to seeing you again for the follow-up.</p>


</section>


<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><a href="https://simonwillison.net/2024/Dec/31/llms-in-2024/#-agents-still-haven-t-really-happened-yet" class="uri">https://simonwillison.net/2024/Dec/31/llms-in-2024/#-agents-still-haven-t-really-happened-yet</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p><a href="https://github.com/huggingface/smolagents" class="uri">https://github.com/huggingface/smolagents</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a href="https://github.com/huggingface/transformers" class="uri">https://github.com/huggingface/transformers</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a href="https://github.com/langchain-ai/langchain" class="uri">https://github.com/langchain-ai/langchain</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><a href="https://huggingface.co/docs/smolagents/conceptual_guides/intro_agents#code-agents" class="uri">https://huggingface.co/docs/smolagents/conceptual_guides/intro_agents#code-agents</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p><a href="https://huggingface.co/blog/open-deep-research" class="uri">https://huggingface.co/blog/open-deep-research</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><a href="https://github.com/stanfordnlp/dspy" class="uri">https://github.com/stanfordnlp/dspy</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p><a href="https://arxiv.org/pdf/2310.03714" class="uri">https://arxiv.org/pdf/2310.03714</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/alexclaydon\.dev");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright 2025, Alex Claydon</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>
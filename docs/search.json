[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Alex Claydon is Head of Data and AI Strategy at Lexical Labs. He is also an Australian Lawyer with over 15 years experience in project finance.\n\n\n Back to top"
  },
  {
    "objectID": "posts/2025-04-09/index.html",
    "href": "posts/2025-04-09/index.html",
    "title": "Relevant Context, Part 2: Ingestion",
    "section": "",
    "text": "“Ingestion” generically refers to the process of transforming a source document into a target format without - or minimising - loss of information.\nThe dominant document formats used for commercial legal agreements are Microsoft Word and Adobe PDF. Adobe’s PDF format can be further subdivided into “text”-based - where the characters on each page are represented as unicode characters - and “image”-based - where the contents of each page are effectively represented as an image file that must be passed through an optical chartacter recognition (OCR) process as part of the ingestion process. Each of these formats presents their own unique challenges.\nReliable, high-quality, at-scale ingestion of the wide variety of document formats in common use today is an enduringly challenging problem - so much so that it continues to support a sizeable enterprise market of sustainably profitable ingestion-as-a-service offerings tailored to specialist domain niches.\nOn the other hand, there are now a number of mature open source projects in the space that have changed what is achievable for smaller teams on more constrained budgets. With modest engineering resources, it is now possible - although it remains non-trivial - to set up and maintain a custom in-house ingestion pipeline for your particular domain."
  },
  {
    "objectID": "posts/2025-04-09/index.html#representations-suitable-for-use-with-llms",
    "href": "posts/2025-04-09/index.html#representations-suitable-for-use-with-llms",
    "title": "Relevant Context, Part 2: Ingestion",
    "section": "Representations Suitable for Use With LLMs",
    "text": "Representations Suitable for Use With LLMs\nBinary formats (such as .docx and .pdf) cannot be directly used in an LLM context window: they must first be transformed into a pure text representation. But that does not imply that we are constrained to only representing textual information: document structure and formatting from the source document - elements of which are just as important for human comprehension as the text itself - can also be retained to the extent that they may be represented using text.\nEven binary formats used in “what you see is what you get” (WYSIWYG) editors such as Microsoft Word represent some of their structure in text directly visible to the end-user: clause numbering in a legal contract, for example. But things like bolding, heading sizes, page breaks, and so forth, are represented using non-textual means - effectively a separate metadata layer which sits alongside the text itself, rather than existing as part of it. Bits of structure and formatting which may be essential to the interpretation of their subject text will be lost to the extent that they cannot be conveted to a textual representation during the ingestion process.\nHappily, there now exist a number of mature, comparatively standardised, text-only representation formats. All of these are in principle suitable for use in LLM context windows, with the better-known variants being naturally preferred for their ubiquity in LLM training datasets. These formats enable varying levels of structural and formatting complexity depending on the particular exigencies of your use-case.\nPerhaps best known - even outside of technical circles - is Markdown, which has been around since 2004 and was formalised into the CommonMark specification in 2014. Markdown’s biggest strength is its simplicity: the specification can be learned in afternoon. But simplicity is also its biggest limitation: primarily focussed on web publishing, Markdown struggles with representations of document structure which go beyond simple heading-level demarcation. Although it has been extended to meet some of these challenges, it remains a fundamentally limited choice for high-fedility, pure-text representation of legal contracts. While it has the twin advantages of being both human-readable and well-understood by LLMs, it fundamentally is not a good choice for a canonical textual representation of a legal contract.\nIt can be an excellent choice for certain LLM-backed point solutions where structure and formatting are less important; but in those cases, you should choose to transform a higher-quality, “lossless” representation of your document into Markdown as and when needed, rather than use Markdown itself as the canonical representation. Unlike transforming proprietary binary formats to pure-text representations, transforming between text-based representations is usually trivial."
  },
  {
    "objectID": "posts/2025-04-09/index.html#canonical-representation",
    "href": "posts/2025-04-09/index.html#canonical-representation",
    "title": "Relevant Context, Part 2: Ingestion",
    "section": "Canonical Representation",
    "text": "Canonical Representation\nPerhaps the best-known alternative pure-text representations are Asciidoc and reStructuredText, both of which are better suited to preserving complex document structure while remaining human-legible. LLMs also appear to be comfortable with XML-based formats - although the trade-off there is perhaps reduced legibility for humans. Should we pick one of these and call it a day?\nThere may seem a neat symmetry between ingesting into, and storing your documents in, the same format that will ultimately be used with your LLM-backed features. But there are some big trade-offs to this approach, and we think many of the benefits are largely illusory, given the existence of better alternatives.\nLet’s take a step back and think about what we’re trying to achieve and what we’ve established:\n\nWe want bulletproof reliability in the ingestion process: getting great results from our product features is hard enough without having to deal with junk input data created by a poor ingestion process.\nWe want to use our documents with LLMs without sacrificing important structure and formatting, certainly.\nHowever, we also know that we will have uses for our documents which do not directly - or immediately - involve LLMs, and for which we may want to leverage document structure or features that cannot be represented in a simple text representation at all. Chief amongst these are what we have referred to in Part 1 of this series as “Document and Chunk Enhancement” processing.\nWe will almost certainly also want to be able to work with the document, or parts of the document, as structured objects in code, rather than just partial strings.\nImages cannot be losslessly represented using pure-text representations, and tables can be difficult to impossible to represent, depending on their complexity.\nTransforming between “near-text” representations and Markdown / Asciidoc / reStructuredText is usually trivially easy - although it may not be lossless, depending on the target format chosen.\n\nLaying it all out like that, it’s clear that our actual constraint isn’t that meaningful document structure and formatting must be immediately representable in pure-text, but that it must be trivial to produce arbitrary pure-text representations from whatever interim canonical format we do choose to use. Loosening the requirement to store our canonical ingested documents in plain text means we can lean into a whole host of extra-textual features, allowing us to store and make sense of images and tables, work with structured objects in code, produce different target transformations for different purposes, and leverage the richer structure and formatting we have captured during ingestion in our metadata extraction and enhancement activities. We should assume that any format with these features is a reasonable candidate for our canonical representation, and indeed we should be ambivalent to any format possessing these features, all else being held equal.\nAs such, we should be much more interested in the robustness of the transformation performed by our ingestion software - including its performance across the major source formats commonly used in our domain - than the particulars of the chosen representation format (assuming it is high-resolution)."
  },
  {
    "objectID": "posts/2025-04-09/index.html#recommended-tooling",
    "href": "posts/2025-04-09/index.html#recommended-tooling",
    "title": "Relevant Context, Part 2: Ingestion",
    "section": "Recommended Tooling",
    "text": "Recommended Tooling\nOwing mainly to how challenging ingestion can be - particularly for documents which require OCR - historical limitations in the performance of open source tooling in the space, and unique business needs, in-house ingestion pipelines tend to involve a complex patchwork of processes. Despite recent advances - particularly in open source tooling - there is no single “off the shelf” tool that is capable of handling all of the edge cases even a team is likely to encounter when working with legal documents.\nThe following are our own recommendations for how we would put together a “greenfield” ingestion pipeline leveraging the current state of the art if we were starting from scratch today. We will update the below as and when there’s anything to add; notably, some of the LLM-backed solutions to knotty edge cases are evolving rapidly - we can reasonably expect to continue to see progress in the coming months.\n\nMicrosoft Word Documents, Adobe PDFs Not Requiring OCR\nIn our view, the starting point for most document ingestion should be either the Apache Foundation’s Tika, or IBM Research Zurich’s Docling. Tika is by far the more mature option, but Docling - which might be a better fit for smaller teams given its more hands-off, opinionated approach - is a serious contender, in our experience demonstrating generally superior handling of PDFs, as well as providing structured extraction of tables and other PDF document features that Tika does not. We also like the structured object produced by Docling and its lossless JSON serialisation option. Some have found that Tika outperforms Docling on Microsoft Office documents, although our own experience is mixed. You should trial both for your use-cases.\nThere are other options - most notably, perhaps, Microsoft’s own MarkItDown - but we are not recommending those at this time, since Tika and Docling - or some combination of the two - are such a compelling starting point.\n\n\nDocuments Requiring OCR\nThe complexity of OCR is driven in part by the wide variation in quality of the source documents. Printing quality, document age, whether the document has itself been reproduced from a poor copy (e.g., a Xerox of another document) - all impact legibility. As such, consistent, reliable, OCR remains challenging, with many edge cases.\nAs with non-OCR PDFs, Tika and Docling are good starting points, both coming bundled with OCR capabilities. Docling is also extensible, and is leaning into the nascent practice of leveraging multimodal LLMs to improve OCR outcomes. Unlike some of the foundation model options below, which are only available over API, SmolDocling-256M-preview can be run on your own hardware. In the context of legal document ingestion, the privacy benefits of this are hard to overlook.\nTo the extent that either of those aren’t suitable for your use-case, we’ve also observed others having some success using freestanding multi-modal foundation LLMs over API as their primary ingestion tool. This approach has privacy implications, given that you will be sending the contents of each page over the wire to OpenAI, Google, etc. In addition, it can be challenging to preserve complete document structure using these approaches. But if all you really need is the text, and you have non-sensitive documents and/or a trusted inference provider, these are good “nuclear” options: they require lots of compute, but - if correctly implemented - can be very reliable. Certainly not something you’d want to use for every document coming in, but something perhaps to hold in reserve for where the cost is justified.\nIn practical terms, this can be done as follows:\n\nExtract each page of your PDF into a separate base-64 encoded image file (using, e.g., ImageMagick)\nIf necessary, de-skew and/or crop the images (again, using, e.g., ImageMagick)\nPass the resulting image binary to one of the following multi-modal LLMs, all of which are considered state of the art for text extraction tasks: Google’s Gemini Flash 2.0, Anthropic’s Claude Sonnet 3.7 or OpenAI’s GPT-4o., along with a prompt instructing the model to extract the text verbatim.\nStitch the resulting output back into a text-only representation (since much of the document structure will have been lost in the conversion)\n\nNote that Mistral also has a specialised endpoint available for OCR-related tasks. We haven’t personally used it, and it faces the same privacy-related concerns as any other LLM over inference API, but it does claim better handling of document structure versus the general-purpose multi-modal LLMs considered above.\nFinally, if you have the hardware for it and can’t send your documents to your API inference provider, Qwen 2.5 32b is widely considered to be state of the art for text extraction tasks within the constraint of common destkop GPU VRAM limits."
  },
  {
    "objectID": "posts/2025-04-02/index.html",
    "href": "posts/2025-04-02/index.html",
    "title": "Agentic RAG, Part 1: A Primer",
    "section": "",
    "text": "Swapping single LLM calls for LLM-backed “agents” leads to materially improved outcomes on retrieval-augmented generation (“RAG”) tasks for legal document analysis. While additional costs are incurred, the approach is already economic for a wide range of B2B use cases and we expect token costs to continue trending downward. The substitution itself is relatively uncomplicated from an engineering standpoint, and eases the prompt engineering burden on domain experts - especially in smaller teams. While there is an impact on inference latency (that needs to be managed in the user experience), the improved outcomes are more than worth it for high value, high risk use cases such as legal analysis.\nThis post - Part 1 - aims to help readers situate agentic RAG in their product stack by considering technological constraints, trade-offs and the likely direction of developments going forward. Part 2, to follow, will compare and contrast two illustrative implementations of agentic behaviour in the context of legal document analysis using the smolagents and DSpy libraries."
  },
  {
    "objectID": "posts/2025-04-02/index.html#context-and-technological-constraints",
    "href": "posts/2025-04-02/index.html#context-and-technological-constraints",
    "title": "Agentic RAG, Part 1: A Primer",
    "section": "Context and Technological Constraints",
    "text": "Context and Technological Constraints\nLLM “agent” frameworks built around large parameter-count foundation model inference over API received a lot of attention during 2023-2024. As with LLMs more generally, users were happy to trade the customisability and privacy of open source models and in-house inference stacks for the vastly more capable larger models and the freedom from having to run their own inference stacks on scarce hardware. Consumer GPU supply was still constrained by pandemic-era knock-on, and most new production capacity was (and is) going to data centre cards. GPT3.5 over API was often radically cheaper than running any of Meta’s early Llama models in-house, for instance, as well as being more performant. That cost equation remains true today, although smaller open source models in the hands of knowledgeable practitioners are now genuinely useful for a variety of use cases.\nOther trade-offs were also accepted - the lack of context-free grammar support, control over logit bias, and the entire post-training and RLHF process, for instance - because the performance gap was so compelling. Function calling support, in particular, was crucial to bridging the gap. While reliability was initially an issue, those concerns have now largely been put to rest even for lower parameter-count LLMs.\nThe effect of these conditions on efforts to spin up useful LLM-backed agents has only recently become clear, and may provide an interesting signal as to where the current big model and inference providers - such as OpenAI - see value being created and captured."
  },
  {
    "objectID": "posts/2025-04-02/index.html#why-agency-matters",
    "href": "posts/2025-04-02/index.html#why-agency-matters",
    "title": "Agentic RAG, Part 1: A Primer",
    "section": "Why “Agency” Matters",
    "text": "Why “Agency” Matters\nAttempts to define what constitutes an “agent” under the new regime are famously fraught.1 The blue-sky prospects are intriguing, but when it comes to using LLMs in legal document analysis, arguably the most compelling application in the present is in improving RAG outcomes.\nWe’re working on a separate post on the merits of RAG systems versus alternative approaches and on preparing your legal documents for use with such systems. For the purposes of this post, suffice to say that the inherent ability of agents to “reflect” and course-correct during the analytical process is an excellent mitigant to the inherent limitations of even current state-of-the-art approaches to RAG.\nAgentic RAG is robust to:\n\na wide variety of applications;\ncompared to single LLM calls, a wider range of approaches to document preparation; and\ncompared to single LLM calls, a wider range of variation in input prompt quality,\n\npotentially allowing teams to depreciate engineering and implementation costs across a range of product features.\nThe principal downsides are:\n\nincreased latency; and\nincreased inference costs,\n\nalthough we expect technological advances will continue to improve both of these.\nGiven the ease of substituting existing RAG systems with an agentic RAG approach, we expect agentic RAG to largely replace most current approaches based on carefully crafted system prompts and single LLM calls in most quality-sensitive domains."
  },
  {
    "objectID": "posts/2025-04-02/index.html#comparison-with-single-call-approaches",
    "href": "posts/2025-04-02/index.html#comparison-with-single-call-approaches",
    "title": "Agentic RAG, Part 1: A Primer",
    "section": "Comparison with Single-Call Approaches",
    "text": "Comparison with Single-Call Approaches\nAll of which papers over perhaps the biggest technical question: how does it work?\nA good starting point might be comparison against canonical “single LLM call” approaches to analystical queries in the legal domain. A user is interested in knowing something about a document that requires more than just searching: they need some kind of synthesis, and analysis, of the contents of the document or documents in order to satisfy an information need.\nA domain specialist - a lawyer or legal engineer, perhaps - will have prepared the model in advance by carefully crafting a system prompt defining a common (as in: used for a variety of applications) specification to help guide the model’s spend of reasoning tokens: “Do this, don’t do that; focus on this, ignore that; contracts provided to you will be governed by English law”. More sophisticated approaches may craft this “framing” prompt dynamically, from information extracted from the document in advance of the user sitting down to interact with the system: “The parties are A and B, incorporated in the United Kingdom; the subject matter is a lease of commercial property located in Manchester, England; disputes shall be resolved through arbitration”. This lightens the load on the end user, who can simply sit down and say: “Summarise the payment terms with a particular focus on the consequences of a failure to pay on time”.\nThe process may not literally be single-call: many systems are likely to perform some degree of error correction and/or post-processing on the result sent back by the LLM. The more cycles and tokens you spend on error correction and post-processing, the more a single-call approach starts to resemble a “loop” in an “untrained” agentic system - particularly if control over the determination of when a response is ready to be seen by the user is largely handed over to the model. So in reality, when it comes to untrained agent systems, there’s a spectrum, with agentic systems handing near-complete control over decision-making to a “managing” agent. “Trained” agent systems are altogether different - something we’ll explore towards the end of the article.\nCareful crafting of the system prompt remains an important step. Teams can and should be creating context and/or document specific framing prompts dynamically from information pre-extracted from the document(s). But by empowering the agent to more aggressively decompose the problem and make active judgements about quality - as we will see below - results in materially improved outcomes and consistency.\nWe’ll consider a concrete implementation of an untrained agent below, but first, it’s worth looking at developments in the LLM agent space in the first few months of 2025, which have thrown into sharp relief an emerging two-track system for LLM agents and a concerted attempt by model and inference providers - OpenAI in particular - to try to re-capture some of the value lost to the preponderance of high-quality open-source models."
  },
  {
    "objectID": "posts/2025-04-02/index.html#agency-untrained-or-trained",
    "href": "posts/2025-04-02/index.html#agency-untrained-or-trained",
    "title": "Agentic RAG, Part 1: A Primer",
    "section": "Agency: Untrained or Trained?",
    "text": "Agency: Untrained or Trained?\nLimited foundation model customisability has led to most popular open-source frameworks modelling their agentic approaches as directed cyclic or acyclic graphs - an approach common in robotic process automation (RPA) and workflow systems. This is effectively the “untrained agent” end of the spectrum discussed above.\nDifferent frameworks abstract different aspects of the process of building the graph, so to speak, but all of them rely on establishing and exploiting a manager/managed relationship between multiple unrelated LLM call contexts. Though these approaches have proven fruitful, they create agency through artifice: the models themselves have no particular internal specialisation in performing agentic behaviours, so we create the desired effect with duelling tokens. Encapsulating relevant context analysis behind the “managed” LLM’s context window, and the output approval process behind that of the “manager” LLM is part of the point, but it is not without trade-offs. The extent of those trade-offs depends in part on the concrete implementation and use-case, and can be hard to measure empirically. But until this year, the point was moot: there was no other way for end-users of the most powerful LLMs provided over inference API to approach the task.\nOn 2 February 2025, OpenAI introduced the “Deep Research” mode in their B2C-facing ChatGPT apps. The rollout to Pro and Plus customers only finished recently, on 25 February. With the caveat that we cannot know for certain how OpenAI is doing what they do - since Deep Research was developed, like all of OpenAI’s products, behind closed doors - informed consensus is that the approach likely involves post-training the LLM with a reward function to select for, and enhance, desirable agentic behaviours such as reflection and error correction as part of the token generation process. Unlike untrained agents, which produce one “cycle” of thought and then are asked by a separate LLM to assess and reconsider their output, the process in trained agents occurs internally: from the user perspective, a question is asked and a high-quality response returned - albeit with higher latency than in single-call scenarios. Decisions on how to improve a piece of analysis, and when it is good enough to return to the user, are all handled by the same LLM, in the same context window.\nIt is early days for Deep Research. Competitors such as Google and Perplexity have released product features with similar names and behaviour that appears superficially similar to OpenAI’s offering. While it is too early for empirical confirmation, subjectively, the ouput produced by a trained agent - or, at least, OpenAI’s Deep Research - is head and shoulders above anything currently possible with untrained agents. OpenAI has not indicated whether a “Deep Research”-like feature will be made available through its API offering. All current signs seem to indicate that features like this represent OpenAI’s attempt to move up the value stack and claw back some of the value lost competitors and the recent spate of incredibly capable open-source models. As such, we would not be surprised if Deep Research continues to be absent from the API for the foreseeable future - at least until another provider commoditises a similarly compelling feature. We think it unlikely that features like this will form the basis of a sustainable moat for OpenAI. The story of the past two years has been that well-capitlised pathfinders like OpenAI are quickly swamped by fast-followers such as Meta and DeepSeek. But, at least for now, those of us implementing agentic behaviours into our products - including for RAG - must make do with existing untrained approaches.\nYou can find interesting in-depth discussion of the “trained vs untrained” dichotomy, and what it may mean for value creation and capture going forward, here and here."
  },
  {
    "objectID": "posts/2025-04-02/index.html#untrained-agent-implementations",
    "href": "posts/2025-04-02/index.html#untrained-agent-implementations",
    "title": "Agentic RAG, Part 1: A Primer",
    "section": "Untrained Agent Implementations",
    "text": "Untrained Agent Implementations\nIn our next post, we propose to walk through two contrasting implementations of agentic RAG in some detail, in the hope that readers may find the concrete examples useful in implementing their own agentic RAG pipelines.\nThere are far too many open-source frameworks enabling agentic behaviour (for some definition of “agentic”) to cover in detail in one post. In any event, the similarity of approaches - a product of them all being confined to “untrained” approaches - would render such exercise of dubious value.\nInstead, we intend to take a high-level look at just two: Hugging Face’s smolagents, and Stanford NLP’s DSpy.\nsmolagents2 is notable for 3 reasons. First, Hugging Face is a giant in the open source machine learning space, thanks in large part to its widely used transformers3 library and its community model and dataset sharing platform, Hugging Face Hub. Second, smolagents itself is committed to being a “low-abstraction” library - unlike, say, LangChain4. This makes it a good fit for prototyping, easy to integrate and - most importantly for our purposes - the code is easy to read and easy to reason about. Third, smolagents takes a somewhat novel approach to writing tool definitions that readers may not have seen before5: instead of JSON, actions are written in Python code. In addition, Hugging Face recently published a blog post covering their own toy untrained re-implementation of OpenAI’s Deep Research feature in the smolagents library.6\nStanford NLP’s DSpy7 is interesting for different reasons, with the chosen approach contrasting nicely with smolagents. DSpy was launched in late 2023 with an Arxiv paper8 that made waves. DSpy’s main value proposition has always been its focus on automated pipeline and prompt optimisation. DSPy approach is proving particularly impactful in the context of setting up untrained agents - including for RAG. Since the links between “manager” and “managed” LLMs in an agentic arrangement are, essentially, prompts, the brittle nature of prompts produced using traditional prompt engineering techniques has weighed heavily on performance.\nWe look forward to seeing you again for the follow-up."
  },
  {
    "objectID": "posts/2025-04-02/index.html#footnotes",
    "href": "posts/2025-04-02/index.html#footnotes",
    "title": "Agentic RAG, Part 1: A Primer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://simonwillison.net/2024/Dec/31/llms-in-2024/#-agents-still-haven-t-really-happened-yet↩︎\nhttps://github.com/huggingface/smolagents↩︎\nhttps://github.com/huggingface/transformers↩︎\nhttps://github.com/langchain-ai/langchain↩︎\nhttps://huggingface.co/docs/smolagents/conceptual_guides/intro_agents#code-agents↩︎\nhttps://huggingface.co/blog/open-deep-research↩︎\nhttps://github.com/stanfordnlp/dspy↩︎\nhttps://arxiv.org/pdf/2310.03714↩︎"
  },
  {
    "objectID": "posts/2025-04-07/index.html",
    "href": "posts/2025-04-07/index.html",
    "title": "Relevant Context, Part 1: Introduction",
    "section": "",
    "text": "LLMs are capable of supporting unprecedented new capabilities for legal document analysis. Tasks which were previously beyond even the most cutting-edge AI tools are becoming feasible: automated document summaries, risk analysis (e.g., against arbitrary playbooks) and real-time document Q&A, for instance, are already available to all of our customers.\nThe the principal challenge for all these use cases - and almost all others involving LLMs in the legal domain - is, and will for the foreseeable future remain, how to get relevant context to the LLM.\nThis post is the first in a planned series exploring practical and workable approaches to the systematic provision of relevant context to LLMs for legal document analysis. The posts will be technical in nature and will focus on current state-of-the-art approaches. We intend to tackle blind spots and difficult trade-offs head-on, rather than hand-waving away the many complexities of this fast-moving area."
  },
  {
    "objectID": "posts/2025-04-07/index.html#scope",
    "href": "posts/2025-04-07/index.html#scope",
    "title": "Relevant Context, Part 1: Introduction",
    "section": "Scope",
    "text": "Scope\nWhat constitutes “relevant context” in the context of legal analysis - indeed in any information retrieval context - can be tough to pin down. It is a function of the question being asked, the identity of the person or party asking it, the nature or type of document or documents in question, and myriad other epistemological factors.\nWhile all interesting questions, for practical purposes we will avail ourselves of a proxy: context is “relevant” if a reasonable legal practitioner would consider it so. Note that we need to ensure both that no relevant context is missing (false negatives) and, ideally, that we don’t pollute the LLM’s context window with irrelevant context (false positives). In practice, LLMs are more robust to false positives: as such, where trade-offs between the two are required, we should favour approaches which minimise false negatives.\nEqually, there are open questions around “how” LLMs make use of the context provided to them: do they prefer certain formats? Does the order in which context is presented matter? Should we - and, if so, how should we - “transform” context to make it more LLM-friendly? As LLMs are ablative black boxes, we don’t have good direct answers to those questions1; instead, we rely on indirect means: observational research (including by third parties) and extensive in-domain permutation testing performed as part of Lexical Labs’ comprehensive in-house legal evaluations (“evals”) programme.\nThose questions warrant their own in-depth posts, but to keep this series applicable to as wide a range of practitioners as possible, we again choose to work with a proxy - one that is reasonably well-aligned with the fundamental architecture of LLMs as stochastic next-token predictors trained on a corpus of natural language text: we assume that LLMs are pareto-effective when receiving information presented in a form that is legible and comprehensible to a human. For example, a human legal practitioner may have expections as to where certain bits of information are located in a legal contract (parties and definitions up front, schedules in the back); our working assumption for the purpose of these posts is that using the existing inherent structure of a document will not materially prejudice LLM comprehension versus alternative hypothetical approaches. It’s an assumption we’ll revisit in detail in later technical posts - in the context of knowledge graphs in particular."
  },
  {
    "objectID": "posts/2025-04-07/index.html#topics-covered",
    "href": "posts/2025-04-07/index.html#topics-covered",
    "title": "Relevant Context, Part 1: Introduction",
    "section": "Topics Covered",
    "text": "Topics Covered\nWhen considering what ground to cover in this series, it is perhaps helpful to think at different levels of abstraction about what we’re trying to achieve.\nAt the highest level, the task for any LLM-backed legal document analysis system is to take a document (or documents), a user query about the document, and respond in a manner that answer’s the query to the satisfaction of the user.\n\n\n\n\n\nflowchart LR\n  A(Query)\n  B(Document)\n  C{LLM}\n  A --&gt; C\n  B --&gt; C\n  C --&gt; D(Answer)\n\n\n\n\n\n\nThe first question that presents itself is: which document? For any query which may be answered with certainty without going outside the bounds of a single, identifiable, document, this looks easy enough. But any scalable systemic approach to AI document analysis must be capable of operating across more than one document.\nAnd since we know that LLM context windows are not now, nor are they ever likely to be, infinite, and that the ability of an LLM to leverage all provided tokens appears to drop off asymptotically as context window size increases as an inherent consequence of the transformer architecture, we can derive from first principles that we should expect superior results from maximising relevant context and minimising pollution of the context window with irrelevant context. That result is certainly supported anecdotally by our own real-world experience, and by that of many other parties working with LLMs on a daily basis.\nAs such, we can now reframe our original question in more specific terms: which parts, of which documents, should we provide?\n\n\n\n\n\nflowchart LR\n  i(Clause)\n  iv(Clause)\n  vii(Clause)\n  viii(Definitions)\n  ix(Definitions)\n\n  i --&gt; 2\n  iv --&gt; 2\n  vii --&gt; 1\n  viii --&gt; 1\n  ix --&gt; 2\n\n  1[Document]\n  2[Document]\n  1 --&gt; B\n  2 --&gt; B\n\n  A(Query)\n  B(Context)\n  C{LLM}\n  A --&gt; C\n  B --&gt; C\n  C --&gt; D(Answer)\n\n\n\n\n\n\nIt follows that, concretely speaking, the tooling we need to answer this question is: (i) a means of identifying and retrieving relevant documents from an arbitrarily-sized corpus, and (ii) a means of identifying and extracting each relevant part of each such releant document without doing harm to the semantic meaning of that part. This last task is - perhaps intuitively - the trickiest: an indemnity clause can be outright misleading if excised from the context of applicable exceptions.\nFrom this we derive us our list of topics to be covered in this series:\n\nIngestion: Systematically and reliably transforming documents (.docx, .pdf, .xlsx, etc.) into LLM-readable form at scale.\nDocument Enhancement: Extracting existing metadata, and creating new metadata, for use primarily as “facets” to aid the process of discriminating relevant documents from irrelevant documents.\nChunking: The principled splitting of documents into components which stand semantically on their own (or with minimal external context ascertainable in advance).\nChunk Enhancement: Processing document chunks to aid in later retrieval.\nQuery Enhancement: Processing the user’s query to aid in the determination of relevance.\nRetrieval: Identifying and retrieving chunks from documents relevant to, and capable of supporting a satisfactory answer to, the user’s query, and presenting them in a sensible form to the downstream LLM tasked with responding.\n\n\n\n\n\n\n\nNote\n\n\n\nLLMs can prove instrumental in all of these stages - particularly when combined with time-tested data cleaning and preparation techniques.\n\n\nNotably, not covered here is how to make use of the finally composed context in an LLM-backed product or feature. We have a separate series currently underway (Part 1) on one of our own tentpole use-cases: agentic RAG for legal document analysis. We will publish additional posts on discrete use-cases at a later date. While the intended end use-case is not entirely decoupled from your approach to preparing and retrieving documents - and should certainly be factored when assessing the various trade-offs and compromises to be made at each stage - the approaches considerded in this series are useful precisely because they are of general application across a wide range of potential use-cases.\nThere is a further practical reason: production engineering teams already have their hands full with your product: bug fixes, new features, UX design, etc. Having a single, well-designed, legible, pareto-efficient document ingestion and processing pipeline supporting a wide range of product features acknowledges the inherent importance and complexity of the task while respecting the resourcing and organisational challenges that accompany the adoption of a radically new technology.\nLet’s get started."
  },
  {
    "objectID": "posts/2025-04-07/index.html#footnotes",
    "href": "posts/2025-04-07/index.html#footnotes",
    "title": "Relevant Context, Part 1: Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://fortune.com/2025/03/27/anthropic-ai-breakthrough-claude-llm-black-box/↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "alexclaydon.dev",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nWednesday, April 9, 2025\n\n\nRelevant Context, Part 2: Ingestion\n\n\n \n\n\n\n\nMonday, April 7, 2025\n\n\nRelevant Context, Part 1: Introduction\n\n\n \n\n\n\n\nWednesday, April 2, 2025\n\n\nAgentic RAG, Part 1: A Primer\n\n\n \n\n\n\n\n\nNo matching items\n\n Back to top"
  }
]
---
title: "Relevant Context, Part 3: Document Chunking"
draft: true
date: "2025-04-10"

execute:
  freeze: auto
  echo: false
---

Document chunking techniques remain one of the most poorly-understood and overlooked factors affecting LLM performance in a wide range of tasks. Most examples in the wild use chunking strategies which are either hopelessly naive for almost any use case (e.g., arbitrary fixed chunk sizes) or specifically unsuitable for use with legal contracts (e.g., sentence/paragraph chunking). Even more complex strategies - such as generic semantic chunking - often take no account of latent structure present both within, and across (as in the case of a suite of transaction documents), legal contracts.

Chunking strategies must be carefully tailored to domain, use-case and proposed retrieval strategies. Existing chunking strategies used with earlier generations of AI - such as expert systems - will likely need updating for use with LLMs. Building on nearly a decade of experience chunking legal contracts for use with AI, we at Lexical Labs have developed an opinionated approach to chunking for LLMs which has proven to be simple, cheap and robust across a range of legal analysis use cases. (#TODO: describe the approach)

## Technological Context and Constraints

Having grown rapidly throughout 2023-24, the size of LLM input context windows for popular foundation models now seems generally to have settled somewhere between 128k (OpenAI GPT-4o, GPT-4o mini, GPT-4.5 Preview) and 200k (OpenAI o1 and o3-mini, Anthropic Claude 3.7 Sonnet and Claude 3.5 Haiku). There are outliers: Google's Gemini 1.5 Flash and Pro, for instance, accept 1m and 2m input tokens respectively, with a claimed "near-perfect retrieval (>99%)".[^1] These limits are driven by technical and economic constraints.

While limited, current LLM context window sizes are often large enough to receive entire contracts or even small document suites. Given dramatically falling token prices over the past 24 months, it has been reasonably economic to compare a "bulk paste" strategy (often known as "context stuffing") to the more selective approaches enabled by effective document chunking. While appealing in it simplicity, it now appears clear that the two mains trade-offs for context stuffing are substantially worse signal-to-noise ratio - a phenomenon that has come to be known as "context window pollution" - and a tendency to stop processing the document when the first plausible text satisfying the user query is found (a particular challenge where more than one type of clause is present in different locations, or multiple clauses bear on the same single query). The trade-offs are severe enough that context stuffing is generally considered poor practice for client-facing use cases - although it remains a handy tool for prototyping. Even if context stuffing tended to work well over a single document, it would have been impossible to scale up to multiple documents.

Since we must choose _which parts_ of _which documents_ should go into an LLM context window, we now effectively have a search and relevance problem. Search has been an active field of research for more than 50 years. As such, there is a deep body of existing literature and expertise we can draw on to understand the various trade-offs and to guide our choices. We will tackle search and relevance in the context of legal documents in a future blog post, but what concerns us presently for the purposes of document chunking is that if we intend to deploy a strategy that returns _parts of_ documents, we must have a means of splitting those documents in a manner which preserves the standalone semantic meaning of the text in any given "chunk".

::: {.callout-note}
Chunking strategy and search strategy are tightly bound: the best solution to splitting a document comes from understanding how it will be used. A surprising example of this is the influence of emerging agentic search paradigms: the correct way to chunk a document changes somewhat if, for example, you have a reasoning agent capable of, say, looking up definitions, or following cross-references. The additional cost implied by such approaches may well make economic sense - particularly as token costs continue to plummet. We'll go into greater detail in our follow-up post on search and relevance, but the approach outlined herein is fully consistent with both agentic and non-agentic use cases.
:::

## Structuring Documents for Humans

To humans, the structural differences between technical and non-technical documents are perhaps obvious and intuitive. Technical documents usually have some latent, human-readable structure to make them easier to navigate in a non-linear fashion. Legal contracts commonly have a table of contents, a definitions section and various other special purpose sections, as well as a structured approach to referencing other parts of the same document - or other related documents - using cross-references. These affordances explicitly acknowledge the role of human-readable structure in both improving the economics of analysing a document by making it easier and faster to find what you need, and reduce the risk of inadvertent drafting errors and omissions.

::: {.callout-note}
One of the great advancements to come out of the LLM revolution is that we no longer need to rely on pre-existing document structure, and can instead extract or create our own depending on need. Possibly the most widely discussed example of this is the renewed interest in knowledge graphs, identified by leading market research firm Gartner as a pivotal technological breakthrough enabled by LLMs.[^2] Knowledge graphs and other approaches to extracting novel structure from legal documents will be the subject of a separate future post.
:::

The practical effect of drafting in such an organised manner is that we are making active decisions about where to situate meaning within a document. By grouping provisions of a certain type together in a single physical location, we create "clusters" - clusters of similar things, perhaps, or of things that logically go together, or perhaps which all relate to a particular part of the document located elsewhere. The intention both is that the practitioner is empowered to jump to the location in the document where they expect to find the information they are looking for - rather than start at page one and read linearly, as with, for example, a novel - but also that looking in that one place implies that if the thing you are looking for that _should_ be there is _not_ there, then you can safely assume that it is _not in the document_.

But a decision to arbitrarily group certain things - certain "units of meaning", or "semantic units" - carries trade-offs. To privilege one dimension is to prejudice another. Should exceptions to a clause be located alongside that clause? Or in a separate "Exceptions" clause? Presumably the answer would be affected by whether the exceptions also applied to _other_ clauses in the document. When should we extract a defined term and when should we not? External exigencies also shape the latent structure of legal contracts - many of them having nothing to do with improving readability: convenience, cost, market convention, the existence of precedent, etc. Lawyers make these value judgements based on experience and convenience. And the established conventions - a single "Definitions" section located at the start of the contract, for example - persist both because they are genuinely useful to human readers, and because it is difficult to do better for something that needs to be consumed as a physical artifact by a human reader. Note that a human reading the same document in electronic form has some additional tooling to help them locate relevant information - the most obvious example being the ability to search for particular words or phrases from their document editor, but the document must remain consumable by a human as a physical artifact.

Documents are fundamentally shaped by the way they are intended to be consumed. Making a document "machine-readable" has always involved transformation - the most fundamental example being the transformation from ink to bits. Documents fed into AI expert systems have also always undergone a transformation process. This process traditionally was handled by specialists and usually more concerned with identifying lexical ("word"-based) instead of semantic ("meaning"-based) features, since semantic understanding has always been the exclusive domain of intelligence.

The advent of LLMs has proven disruptive. To a human, the process of feeding a document to an LLM looks, superficially, like giving it to a human reader - a process perhaps reinforced by the use of chat as the dominant medium for interacting with LLMs. But LLMs do not "read" documents in any way that we understand or intend the use of that term - a fact trivially confirmed by even casual observation of the most common failure modes using LLMs for document analysis. This misconception is a large reason for the mixed results produced by common approaches to both context stuffing and "retrieval-augmented generation" ("RAG").

Since we do not know _how_ LLMs interpret and construct meaning (the debate rages on), all we can do - at least for now - is empirically confirm through extensive testing which transformation modalities produce consistently desirable and valuable output for our use cases. Our options span a range of levels of effort, from quick and dirty to expensive and complex.

We now have two meaningful drivers for selection of our context mangement strategy. We can add to that a third: effectiveness. This gives us the following plot. This gives us a plot of where each technique should be used and how effective it is.

```{python}
import random
from typing import Iterable

import matplotlib.pyplot as plt

def plot_3d_scatter(points: Iterable[list[str, int, int, int]]) -> None:
    fig = plt.figure()
    ax = fig.add_subplot(projection='3d')
    x_coords = [p[1] for p in points]
    y_coords = [p[2] for p in points]
    z_coords = [p[3] for p in points]
    ax.scatter(x_coords, y_coords, z_coords)
    ax.set_title(" " * 15 + "Contrasting Context Management Strategies" + " " * 15)
    ax.set_xlabel("Transformation Effort")
    ax.set_ylabel("Retriever Agency")
    ax.set_zlabel("Efficacy", rotation=90)
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_zticks([])
    for label, x, y, z in points:
        ax.text(x, y, z, label)

    plt.show()

data = [
    [" Context Stuffing", 1, 2, 3],
    [" Recursive Semantic Chunking", 5, 1, 3],
    [" Per-Clause Chunking", 2, 5, 4]
]
    
plot_3d_scatter(data)
```

A further consideration is whether repetition can be a useful tool here

::: {.callout-note}
All of the above is further complicated by technical challenges in parsing documents to a machine-readable form. PDFs - particularly those which require optical character recognition - remain challenging even today. While some newer LLM-backed solutions (including using Gemini Flash in the ingestion pipeline) promise a leap over venerable ingestion technologies such as Apache Tika[^3] and more recent entries such as IBM Research Zurich's Docling[^4] and Unstructured[^5], there is still a way to go before such solutions are mature enough for production workloads at scale.
:::

## Preserving Meaning While Splitting

The platonic ideal of our goal is to split our contracts in such a manner that, for any given question or use case, we can reliably provide the LLM with all relevant information, and no irrelevant information. In practice, LLMs are more sensitive to the absence of _relevant information_ (false negatives) than to the presence of small quantities of _irrelevant information_ (false positives). So while we can somewhat relax that last condition, no false negatives is table stakes for something as consequential as contracting.

### Document Chunking and Node Chunking for Parent-Document Retrievers

All of the above concerns splitting the document

Embeddings

Sentence chunking.

### Chunking for Agentic and Non-Agentic Use Cases

Front door -> walk through house



[^1]: <https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf>
[^2]: <https://www.gartner.com/en/articles/hype-cycle-for-artificial-intelligence>
[^3]: <https://tika.apache.org/>
[^4]: <https://docling-project.github.io/docling/>
[^5]: <https://docs.unstructured.io/welcome>